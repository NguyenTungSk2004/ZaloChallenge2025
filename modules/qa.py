# qa.py (ƒê√£ s·ª≠a)

import torch
import time
from langchain_community.llms import LlamaCpp
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from sentence_transformers import CrossEncoder

# --- 1-3. LOAD MODELS (Gi·ªØ nguy√™n) ---
# (T·∫£i Embeddings, Chroma, Reranker)
print("üöÄ ƒêang t·∫£i Embedder, ChromaDB, Reranker...")
EMB_PATH = "E:/Zalo Challenge 2025/Build_RAG/model/bkai_vn_bi_encoder"
embeddings = HuggingFaceEmbeddings(
    model_name=EMB_PATH,
    model_kwargs={'device': 'cuda'},
    encode_kwargs={'normalize_embeddings': False}
)
DB_PATH = "E:/Zalo Challenge 2025/module_rag/Vecto_Database/db_bienbao_2"
vectordb = Chroma(
    persist_directory=DB_PATH,
    embedding_function=embeddings
)
retriever = vectordb.as_retriever(search_kwargs={"k": 8})
RERANK_PATH = "E:/Zalo Challenge 2025/Build_RAG/model/ViRanker"
device = "cuda" if torch.cuda.is_available() else "cpu"
reranker = CrossEncoder(RERANK_PATH, device=device)
print("‚úÖ T·∫£i RAG (retriever, reranker) th√†nh c√¥ng.")

def rerank(query, docs, k=3):
    if not docs: return []
    pairs = [(query, d.page_content) for d in docs]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [d for d, _ in ranked[:k]]

# --- 4. LOAD PHI-3 (Gi·ªØ nguy√™n) ---
print("üöÄ ƒêang t·∫£i LLM (Phi-3)...")
LLM_PATH = "E:/Zalo Challenge 2025/Build_RAG/model/Phi-3-gguf/Phi-3-mini-4k-instruct-q4.gguf"
llm = LlamaCpp(
    model_path=LLM_PATH,
    n_gpu_layers=-1, n_batch=512, n_ctx=4096,
    temperature=0.1, top_p=0.9, max_tokens=256,
    verbose=False
)
print("‚úÖ T·∫£i LLM (Phi-3) th√†nh c√¥ng.")

# --- 5. PROMPT (ƒê√£ s·ª≠a) ---
def format_docs(docs):
    out = ""
    for d in docs:
        bien = d.metadata.get("bien_so", "")
        out += f"[Bi·ªÉn s·ªë: {bien}]\n{d.page_content.strip()}\n\n"
    return out.strip()

# C√°c v√≠ d·ª• few-shot (Gi·ªØ nguy√™n)
question_1 = "Trong video, bi·ªÉn b√°o n√†o xu·∫•t hi·ªán ƒë·∫ßu ti√™n?"
choices_1 = """A. Bi·ªÉn gi·ªØ kho·∫£ng c√°ch an to√†n
B. Bi·ªÉn gi·ªõi h·∫°n t·ªëc ƒë·ªô t·ªëi ƒëa
C. Bi·ªÉn c·∫•m d·ª´ng ƒë·ªó
D. Bi·ªÉn c·∫•m xe t·∫£i"""
answer_1 = "A. Bi·ªÉn gi·ªØ kho·∫£ng c√°ch an to√†n"
question_2 = "Bi·ªÉn b√°o 'Stop' y√™u c·∫ßu ng∆∞·ªùi l√°i xe l√†m g√¨?"
choices_2 = """A. Gi·∫£m t·ªëc ƒë·ªô v√† quan s√°t
B. D·ª´ng l·∫°i ho√†n to√†n, nh∆∞·ªùng ƒë∆∞·ªùng
C. Ch·ªâ d·ª´ng l·∫°i khi c√≥ xe kh√°c
D. B·∫•m c√≤i c·∫£nh b√°o"""
answer_2 = "B. D·ª´ng l·∫°i ho√†n to√†n, nh∆∞·ªùng ƒë∆∞·ªùng"


# S·ª¨A L·ªñI QUAN TR·ªåNG: Th√™m placeholders {vlm_context}, {rag_context}, {query}, {choices}
# (L∆∞u √Ω: d√πng {{}} ƒë·ªÉ gi·ªØ ch·ªó cho f-string, v√¨ PromptTemplate s·∫Ω d√πng .format())
TEMPLATE = f"""<|system|>
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng v√† ph√¢n t√≠ch video.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·ªïng h·ª£p B·∫∞NG CH·ª®NG T·ª™ VIDEO (VLM) v√† KI·∫æN TH·ª®C V·ªÄ LU·∫¨T (RAG) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi tr·∫Øc nghi·ªám.
Ch·ªâ tr·∫£ l·ªùi b·∫±ng l·ª±a ch·ªçn ƒë√∫ng (v√≠ d·ª•: 'A. [N·ªôi dung]').
<|end|>

<|user|>
Question: "{question_1}"
Choices:
{choices_1}
<|end|>
<|assistant|>
{answer_1}
<|end|>

<|user|>
Question: "{question_2}"
Choices:
{choices_2}
<|end|>
<|assistant|>
{answer_2}
<|end|>

<|user|>
B·∫∞NG CH·ª®NG T·ª™ VIDEO (VLM):
{{vlm_context}}

KI·∫æN TH·ª®C V·ªÄ LU·∫¨T (RAG):
{{rag_context}}

Question: "{{query}}"
Choices:
{{choices}}
<|end|>
<|assistant|>
"""

prompt = PromptTemplate.from_template(TEMPLATE)

# --- 6. PUBLIC FUNCTION (ƒê√£ s·ª≠a) ---
def lm_generate(vlm_context: str, query: str, choices: list[str]) -> str:
    """
    H√†m public ƒë·ªÉ team g·ªçi t·ª´ pipeline ch√≠nh.
    Nh·∫≠n context VLM, c√¢u h·ªèi, v√† c√°c l·ª±a ch·ªçn.
    """

    # 1. Retrieve (D·ª±a tr√™n c√¢u h·ªèi)
    docs = retriever.invoke(query)

    # 2. Rerank
    top_docs = rerank(query, docs, k=3)

    # 3. Format RAG context (t·ª´ vectordb)
    rag_context = format_docs(top_docs)
    if len(top_docs) == 0:
        rag_context = "Kh√¥ng t√¨m th·∫•y lu·∫≠t li√™n quan."

    # 4. Format choices (t·ª´ input)
    choices_str = "\n".join(choices)

    # 5. Run LLM
    final_prompt_str = prompt.format(
        vlm_context=vlm_context,
        rag_context=rag_context,
        query=query,
        choices=choices_str
    )
    
    print("--- DEBUG: PROMPT CU·ªêI C√ôNG G·ª¨I T·ªöI PHI-3 ---")
    print(final_prompt_str)
    print("------------------------------------------")

    answer = llm.invoke(final_prompt_str)
    return answer.strip()