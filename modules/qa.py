from langchain_core.prompts import PromptTemplate

def rerank(reranker, query, docs, k=3):
    if not docs:
        return []
    pairs = [(query, d.page_content) for d in docs]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [d for d, _ in ranked[:k]]

# 5. PROMPT
def format_docs(docs):
    out = ""
    for d in docs:
        bien = d.metadata.get("bien_so", "")
        out += f"[Biá»ƒn sá»‘: {bien}]\n{d.page_content.strip()}\n\n"
    return out.strip()

TEMPLATE = """
<|system|>
Báº¡n lÃ  trá»£ lÃ½ giao thÃ´ng. Chá»‰ tráº£ lá»i dá»±a trÃªn CONTEXT vÃ  VIDEO DESCRIPTION.
STRICT RULE :
1. Chá»‰ chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng.
2. KhÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm.
3. KhÃ´ng suy diá»…n.
4.Náº¿u khÃ´ng cÃ³ thÃ´ng tin â†’ tráº£ lá»i Ä‘Ãºng cÃ¢u:
"TÃ´i khÃ´ng tÃ¬m tháº¥y biá»ƒn bÃ¡o phÃ¹ há»£p trong cÆ¡ sá»Ÿ dá»¯ liá»‡u."
KhÃ´ng suy diá»…n. KhÃ´ng thÃªm kiáº¿n thá»©c ngoÃ i context.
<|end|>

### FEW-SHOT EXAMPLES ###

<|user|>
video_description: The road section has three lanes. The outermost right lane is a mixed lane for both motorbikes and cars. Ahead, there is a sign allowing vehicles to go straight or turn right.
question: Náº¿u xe Ã´ tÃ´ Ä‘ang cháº¡y á»Ÿ lÃ n ngoÃ i cÃ¹ng bÃªn pháº£i trong video nÃ y thÃ¬ xe Ä‘Ã³ chá»‰ Ä‘Æ°á»£c phÃ©p ráº½ pháº£i?
choices:
- A. ÄÃºng
- B. Sai

<context>FAKE_CONTEXT</context>

HÃ£y chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng.
<|assistant|>
B

<|user|>
video_description: The road section contains three lanes. A traffic sign R.411 ahead shows arrows indicating allowed directions: straight, left turn, and right turn.
question: Pháº§n Ä‘Æ°á»ng trong video cho phÃ©p cÃ¡c phÆ°Æ¡ng tiá»‡n Ä‘i theo hÆ°á»›ng nÃ o khi Ä‘áº¿n nÃºt giao?
choices:
- A. Äi tháº³ng
- B. Äi tháº³ng vÃ  ráº½ pháº£i
- C. Äi tháº³ng, ráº½ trÃ¡i vÃ  ráº½ pháº£i
- D. Ráº½ trÃ¡i vÃ  ráº½ pháº£i

<context>FAKE_CONTEXT</context>

HÃ£y chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng.
<|assistant|>
C

### END FEW-SHOT EXAMPLES ###


### REAL USER QUESTION ###

<|user|>
video_description: {video_description}
{question}

<context>
{context}
</context>

HÃ£y chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng.
<|assistant|>
"""

prompt = PromptTemplate.from_template(TEMPLATE)

# 6. PUBLIC FUNCTION: lm_generate()
def lm_generate(*,llm, tokenizer,retriever, reranker, vlm_description: str, question: str) -> str:
    """HÃ m public Ä‘á»ƒ team gá»i tá»« pipeline chÃ­nh"""

    # 1. Retrieve
    docs = retriever.invoke(vlm_description)

    # 2. Rerank
    top_docs = rerank(reranker, vlm_description, docs, k=3)

    # 3. Kiá»ƒm tra context rá»—ng
    if len(top_docs) == 0:
        return "TÃ´i khÃ´ng tÃ¬m tháº¥y biá»ƒn bÃ¡o phÃ¹ há»£p trong cÆ¡ sá»Ÿ dá»¯ liá»‡u."

    # 4. Format context
    context = format_docs(top_docs)

    # 5. Run LLM
    final_prompt = prompt.format(context=context, video_description=vlm_description, question=question)
    # 5.1. Tokenize prompt
    inputs = tokenizer(final_prompt, return_tensors="pt", return_attention_mask=False).to(llm.device)
    
    # 5.2. Generate answer
    # LÆ°u Ã½: Cáº§n thÃªm max_new_tokens Ä‘á»ƒ giá»›i háº¡n Ä‘á»™ dÃ i cÃ¢u tráº£ lá»i cá»§a LLM
    outputs = llm.generate(
        **inputs,
        max_new_tokens=256, # Giá»›i háº¡n 256 token má»›i cho cÃ¢u tráº£ lá»i
        pad_token_id=tokenizer.pad_token_id, # Cáº§n thiáº¿t cho mÃ´ hÃ¬nh Phi-3
        eos_token_id=tokenizer.eos_token_id
    )

    # 5.3. Decode (Chá»‰ giáº£i mÃ£ pháº§n cÃ¢u tráº£ lá»i)
    # Do mÃ´ hÃ¬nh sinh ra cáº£ prompt, chÃºng ta cáº§n loáº¡i bá» Ä‘á»™ dÃ i cá»§a prompt gá»‘c.
    prompt_length = inputs['input_ids'].shape[1]
    
    # Debug: in ra thÃ´ng tin generation
    print(f"ğŸ” Prompt length: {prompt_length}")
    print(f"ğŸ” Output length: {outputs[0].shape}")
    
    # Giáº£i mÃ£ pháº§n token Ä‘Æ°á»£c sinh ra (sau prompt)
    if outputs[0].shape[0] > prompt_length:
        generated_tokens = outputs[0][prompt_length:]
        answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        print(f"ğŸ” Generated tokens: {generated_tokens[:10]}")  # In 10 token Ä‘áº§u
        print(f"ğŸ” Raw answer: '{answer}'")
    else:
        # Fallback: decode toÃ n bá»™ rá»“i loáº¡i bá» prompt
        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        prompt_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
        
        print(f"ğŸ” Full output: {full_output[:300]}...")
        print(f"ğŸ” Prompt text: {prompt_text[-200:]}")  # In 200 kÃ½ tá»± cuá»‘i cá»§a prompt
        
        if prompt_text in full_output:
            answer = full_output.replace(prompt_text, "", 1).strip()
        else:
            answer = full_output.strip()
        
        print(f"ğŸ” Final answer after cleanup: '{answer}'")

    # LÃ m sáº¡ch answer - chá»‰ láº¥y kÃ½ tá»± Ä‘áº§u tiÃªn náº¿u Ä‘Ã³ lÃ  A, B, C, D
    answer_clean = answer.strip()
    if answer_clean and answer_clean[0] in ['A', 'B', 'C', 'D']:
        answer_clean = answer_clean[0]
    elif 'A' in answer_clean:
        answer_clean = 'A'
    elif 'B' in answer_clean:
        answer_clean = 'B' 
    elif 'C' in answer_clean:
        answer_clean = 'C'
    elif 'D' in answer_clean:
        answer_clean = 'D'
    else:
        print(f"âš ï¸ No valid answer found, defaulting to A. Raw: '{answer_clean}'")
        answer_clean = 'A'
    
    print(f"ğŸ” Final cleaned answer: '{answer_clean}'")
    return answer_clean
