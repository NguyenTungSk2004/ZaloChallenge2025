import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline
from transformers import GenerationConfig

# --- 1. ƒê·ªäNH NGHƒ®A ƒê∆Ø·ªúNG D·∫™N ---
LLM_PATH = "models/google/gemma-3-4b-it" 
KNOWLEDGE_BASE_PATH = "scripts/knowledge_base_final.json" 
OUTPUT_BKAI_TRAIN = "json_file/train_bkai.jsonl"
OUTPUT_PHI3_TRAIN = "json_file/train_phi3.jsonl"

# --- 2. LOAD GEMMA 3 4B ---
print(f"üîÑ ƒêang t·∫£i Model t·ª´: {LLM_PATH}...")
try:
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(LLM_PATH, trust_remote_code=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "left"

    model = AutoModelForCausalLM.from_pretrained(
        LLM_PATH,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=False,
        torch_dtype=torch.bfloat16,
    )

    # --- 1Ô∏è‚É£ T·∫°o GenerationConfig chu·∫©n cho Gemma 3 4B ---
    model.generation_config = GenerationConfig(
        max_new_tokens=384,
        temperature=0.2,
        top_p=0.9,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    print("‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng Model Gemma 3 4B.")

except Exception as e:
    print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ t·∫£i model t·∫°i: {LLM_PATH}")
    print(f"Chi ti·∫øt l·ªói: {e}")
    exit()

# --- 3. H√ÄM TI·ªÜN √çCH ---
def load_knowledge_base(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"‚úÖ ƒê√£ t·∫£i {len(data)} chunks t·ª´ {file_path}")
        return data
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i {file_path}: {e}")
        return None

def generate_qa_for_chunk(chunk_content):
    """
    Version ·ªïn ƒë·ªãnh cho Gemma 3 4B:
    - Model sinh Q&A d·∫°ng plain text
    - H·∫≠u k·ª≥ parse JSON
    """
    messages = [
        {"role": "system",
         "content": (
            "B·∫°n l√† chuy√™n gia t·∫°o c√¢u h·ªèi ‚Äì c√¢u tr·∫£ l·ªùi t·ª´ vƒÉn b·∫£n lu·∫≠t.\n"
            "Nhi·ªám v·ª• duy nh·∫•t: sinh ra ƒë√∫ng 3 c·∫∑p Q&A.\n"
            "M·ªói Q&A ph·∫£i li√™n quan 100% ƒë·∫øn context.\n"
            "Kh√¥ng b·ªãa th√¥ng tin ngo√†i context.\n"
            "Kh√¥ng gi·∫£i th√≠ch, kh√¥ng t√≥m t·∫Øt.\n"
            "Tr·∫£ v·ªÅ ƒë√∫ng format:\n"
            "Q1: <c√¢u h·ªèi 1>\nA1: <c√¢u tr·∫£ l·ªùi 1>\n"
            "Q2: <c√¢u h·ªèi 2>\nA2: <c√¢u tr·∫£ l·ªùi 2>\n"
            "Q3: <c√¢u h·ªèi 3>\nA3: <c√¢u tr·∫£ l·ªùi 3>"
        )},
        {"role": "user",
         "content": f"Context:\n{chunk_content}\n\nH√£y t·∫°o ƒë√∫ng 3 Q&A theo ƒë·ªãnh d·∫°ng."}
    ]

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.inference_mode():
        outputs = model.generate(**inputs)  # ‚ö° Ch·ªâ c·∫ßn inputs, config ƒë√£ g√°n

    generated = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()

    # --- H·∫≠u k·ª≥: parse sang JSON ---
    qa_pairs = []
    lines = generated.splitlines()
    pair = {}
    for line in lines:
        line = line.strip()
        if line.startswith("Q"):
            if pair:
                qa_pairs.append(pair)
            pair = {"question": line.split(":", 1)[1].strip()}
        elif line.startswith("A") and pair:
            pair["answer"] = line.split(":", 1)[1].strip()
    if pair:
        qa_pairs.append(pair)

    qa_pairs = qa_pairs[:3]
    if len(qa_pairs) != 3:
        print("‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c 3 Q&A:", generated[:200])
        return []

    return qa_pairs

# --- 4. H√ÄM CH√çNH ---
def create_and_save_training_files(knowledge_chunks):
    with open(OUTPUT_BKAI_TRAIN, 'w', encoding='utf-8') as f_bkai, \
         open(OUTPUT_PHI3_TRAIN, 'w', encoding='utf-8') as f_phi3:

        print("\nüöÄ B·∫Øt ƒë·∫ßu t·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán...")
        total_qa_pairs = 0

        for i, chunk in enumerate(knowledge_chunks):
            chunk_content = chunk.get("page_content")
            if not chunk_content:
                continue

            print(f"    ƒêang x·ª≠ l√Ω chunk {i+1}/{len(knowledge_chunks)} (ID: {chunk['metadata'].get('doc_id')})...")

            qa_pairs = generate_qa_for_chunk(chunk_content)
            if not qa_pairs:
                print(f"    Kh√¥ng t·∫°o ƒë∆∞·ª£c Q&A cho chunk {i+1}.")
                continue

            for pair in qa_pairs:
                instruction = pair["question"]
                response = pair["answer"]

                # --- BKAI ---
                bkai_data = {"query": instruction, "positive": chunk_content}
                f_bkai.write(json.dumps(bkai_data, ensure_ascii=False) + "\n")

                # --- Phi-3 format ---
                phi3_text = f"<|system|>\nB·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.\n<|end|>\n"
                phi3_text += f"<|user|>\nB·ªêI C·∫¢NH LU·∫¨T:\n{chunk_content}\n\nC√ÇU H·ªéI:\n{instruction}\n<|end|>\n"
                phi3_text += f"<|assistant|>\n{response}\n<|end|>"

                phi3_data = {"text": phi3_text}
                f_phi3.write(json.dumps(phi3_data, ensure_ascii=False) + "\n")

                total_qa_pairs += 1

            print(f"    ƒê√£ t·∫°o {len(qa_pairs)} c·∫∑p Q&A. T·ªïng c·ªông: {total_qa_pairs}")

    print("\n--- HO√ÄN T·∫§T ---")
    print(f"‚úÖ ƒê√£ l∆∞u file training cho BKAI t·∫°i: {OUTPUT_BKAI_TRAIN}")
    print(f"‚úÖ ƒê√£ l∆∞u file training cho Phi-3 t·∫°i: {OUTPUT_PHI3_TRAIN}")
    print(f"T·ªïng c·ªông {total_qa_pairs} m·∫´u hu·∫•n luy·ªán ƒë√£ ƒë∆∞·ª£c t·∫°o.")

# --- 5. CH·∫†Y SCRIPT ---
if __name__ == "__main__":
    kb_chunks = load_knowledge_base(KNOWLEDGE_BASE_PATH)
    if kb_chunks:
        create_and_save_training_files(kb_chunks)
