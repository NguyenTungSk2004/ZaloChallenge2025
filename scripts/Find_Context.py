import json
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from sentence_transformers import CrossEncoder
import torch

# --- 1. T·∫¢I C√ÅC TH√ÄNH PH·∫¶N RAG (T·ª™ qa.py) ---
print("üîÑ ƒêang t·∫£i c√°c model RAG...")
EMB_PATH = "models/bkai_vn_bi_encoder"
RERANK_PATH = "models/ViRanker"
KNOWLEDGE_BASE_PATH = "scripts/knowledge_base_final.json"
PARSED_QUESTIONS_PATH = "scripts/parsed_questions.json"
OUTPUT_GOLDEN_DATASET = "scripts/golden_dataset.json" # File t·ªïng h·ª£p cu·ªëi c√πng

device = "cuda" if torch.cuda.is_available() else "cpu"

# T·∫£i Embedder (BKAI)
embeddings = HuggingFaceEmbeddings(
    model_name=EMB_PATH,
    model_kwargs={'device': device},
    encode_kwargs={'normalize_embeddings': True} # N√™n normalize cho BKAI
)

# T·∫£i Reranker (ViRanker)
reranker = CrossEncoder(RERANK_PATH, device=device)

# --- 2. T·∫¢I V√Ä T·∫†O VECTOR DB T·ª™ JSON ---
print(f"üîÑ ƒêang t·∫£i Knowledge Base t·ª´ {KNOWLEDGE_BASE_PATH}...")
try:
    with open(KNOWLEDGE_BASE_PATH, 'r', encoding='utf-8') as f:
        kb_data = json.load(f) #
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i {KNOWLEDGE_BASE_PATH}: {e}")
    exit()

# Chuy·ªÉn ƒë·ªïi dicts sang Document objects
all_kb_docs = [
    Document(
        page_content=item["page_content"],
        metadata=item["metadata"]
    ) for item in kb_data if "page_content" in item
]
print(f"‚úÖ ƒê√£ t·∫£i {len(all_kb_docs)} chunks lu·∫≠t/bi·ªÉn b√°o.")

print("üîÑ ƒêang t·∫°o ChromaDB (in-memory)...")
# T·∫°o DB t·∫°m th·ªùi trong RAM
vectordb = Chroma.from_documents(
    documents=all_kb_docs,
    embedding=embeddings
)
# T·∫°o retriever (b·ªô t√¨m ki·∫øm th√¥)
retriever = vectordb.as_retriever(search_kwargs={"k": 10}) # L·∫•y top 10

# --- 3. T·∫¢I C√ÅC C√ÇU H·ªéI ƒê√É PARSE ---
try:
    with open(PARSED_QUESTIONS_PATH, 'r', encoding='utf-8') as f:
        questions_data = json.load(f)
    print(f"‚úÖ ƒê√£ t·∫£i {len(questions_data)} c√¢u h·ªèi t·ª´ {PARSED_QUESTIONS_PATH}.")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i {PARSED_QUESTIONS_PATH}: {e}")
    exit()
    
# --- 4. LI√äN K·∫æT CONTEXT V√Ä L∆ØU GOLDEN DATASET ---
golden_dataset = []
print("\nüöÄ B·∫Øt ƒë·∫ßu li√™n k·∫øt context cho t·ª´ng c√¢u h·ªèi...")

for i, item in enumerate(questions_data):
    query = item["question"]
    answer = item["answer"]
    
    # B∆∞·ªõc 4.1: Retrieve (T√¨m ki·∫øm th√¥ b·∫±ng BKAI)
    retrieved_docs = retriever.invoke(query)
    
    if not retrieved_docs:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y context cho c√¢u: {item['id']}")
        continue
        
    # B∆∞·ªõc 4.2: Rerank (Tinh l·ªçc b·∫±ng ViRanker)
    pairs = [(query, d.page_content) for d in retrieved_docs]
    scores = reranker.predict(pairs)
    
    # S·∫Øp x·∫øp v√† l·∫•y context t·ªët nh·∫•t
    ranked = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)
    best_doc = ranked[0][0] # Ch·ªâ l·∫•y 1 context t·ªët nh·∫•t
    
    # T·∫°o b·∫£n ghi m·ªõi
    golden_record = {
        "id": item["id"],
        "query": query,
        "answer": answer,
        "context": best_doc.page_content, # ƒê√¢y l√† "ƒëo·∫°n lu·∫≠t"
        "context_metadata": best_doc.metadata
    }
    golden_dataset.append(golden_record)
    
    if (i+1) % 50 == 0:
        print(f"    ... ƒê√£ x·ª≠ l√Ω {i+1}/{len(questions_data)} c√¢u h·ªèi ...")

print(f"‚úÖ ƒê√£ li√™n k·∫øt context cho {len(golden_dataset)} c√¢u h·ªèi.")

# L∆∞u file "v√†ng"
with open(OUTPUT_GOLDEN_DATASET, 'w', encoding='utf-8') as f:
    json.dump(golden_dataset, f, ensure_ascii=False, indent=4)
print(f"‚úÖ ƒê√£ l∆∞u b·ªô d·ªØ li·ªáu V√†ng (Golden Dataset) v√†o: {OUTPUT_GOLDEN_DATASET}")