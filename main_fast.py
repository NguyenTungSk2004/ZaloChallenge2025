import json
import csv
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
import torch
import pandas as pd
from ultralytics import YOLO
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
    AutoModelForImageTextToText,
    AutoTokenizer,
    BitsAndBytesConfig
)
from sentence_transformers import CrossEncoder

from modules.tracker import BestFrameTracker
from modules.extract_frames import extract_frames_to_queue
from modules.vlm import describe_frame_with_prompt
from modules.qa import lm_generate

# Thread-safe lock
file_lock = threading.Lock()
print_lock = threading.Lock()

def thread_safe_print(*args, **kwargs):
    """In an toÃ n vá»›i Ä‘a luá»“ng"""
    with print_lock:
        print(*args, **kwargs)

def load_models_fast():
    """Load models vá»›i tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™"""
    thread_safe_print("ğŸš€ Äang load models (tá»‘i Æ°u tá»‘c Ä‘á»™)...")
    
    # Clear GPU cache trÆ°á»›c khi load
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # 1. NF4 QUANTIZATION CONFIG
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=False,  # Táº¯t Ä‘á»ƒ tÄƒng tá»‘c
        bnb_4bit_compute_dtype=torch.float16
    )

    # 2. LOAD BLIP2 VLM
    model_path = "models/blip2-opt-2.7b"
    processor = AutoProcessor.from_pretrained(model_path, use_fast=True)
    model = AutoModelForImageTextToText.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16
    )
    model.eval()  # Chá»‰ eval, khÃ´ng .half() cho quantized model

    # 3. YOLO
    model_path_yolo = "models/yolo/best.pt"
    yolo_detector = YOLO(model_path_yolo)
    yolo_detector.model.eval()

    # 4. EMBEDDING + CHROMA
    EMB_PATH = "models/bkai-foundation-models/vietnamese-bi-encoder"
    embeddings = HuggingFaceEmbeddings(
        model_name=EMB_PATH,
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': False}
    )

    DB_PATH = "Vecto_Database/db_bienbao_2"
    vectordb = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings
    )
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    # 5. RERANKER
    RERANK_PATH = "models/namdp-ptit/ViRanker"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    reranker = CrossEncoder(RERANK_PATH, device=device)

    # 6. LOAD PHI-3 MINI
    LLM_PATH = "models/microsoft/Phi-3-mini-4k-instruct"
    tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    llm = AutoModelForCausalLM.from_pretrained(
        LLM_PATH,
        quantization_config=bnb_config,
        device_map="auto",
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16
    )
    llm.eval()  # Chá»‰ eval, khÃ´ng .half() cho quantized model

    thread_safe_print("âœ… Táº¥t cáº£ models Ä‘Ã£ load xong!")
    
    return {
        'processor': processor,
        'model': model,
        'yolo_detector': yolo_detector,
        'retriever': retriever,
        'reranker': reranker,
        'llm': llm,
        'tokenizer': tokenizer
    }

def process_single_question_fast(args):
    """Xá»­ lÃ½ má»™t cÃ¢u há»i vá»›i tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™"""
    question_data, models, question_index, total_questions = args
    
    start_time = time.time()
    
    try:
        # Khá»Ÿi táº¡o tracker
        tracker = BestFrameTracker()
        frames_queue = extract_frames_to_queue(question_data['video_path'])
        
        frame_count = 0
        skip_factor = 2  # Skip má»—i frame thá»© 2 Ä‘á»ƒ tÄƒng tá»‘c
        
        while True:
            frame = frames_queue.get()
            if frame is None:
                break
                
            frame_count += 1
            
            # Skip frames Ä‘á»ƒ tÄƒng tá»‘c
            if frame_count % skip_factor != 0:
                continue

            # YOLO detection vá»›i tá»‘i Æ°u
            with torch.no_grad():
                results = models['yolo_detector'].track(frame, tracker="bytetrack.yaml", verbose=False)
                
            if not results or len(results) == 0:
                continue

            # Cáº­p nháº­t tracker
            for box in results[0].boxes:
                if box.id is None:
                    continue

                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                track_id = int(box.id)
                conf = float(box.conf)
                cls_id = int(box.cls)
                cls_name = results[0].names[cls_id]

                tracker.update_track(frame, track_id, (x1, y1, x2, y2), conf, cls_name)

        # VLM Captioning nhanh
        all_caption = ""
        for track_id, frameData in tracker.best_frames.items():
            box = frameData.box_info

            # Prompt ngáº¯n gá»n
            prompt = f"Traffic sign text/symbol in {box.bbox}:"

            with torch.no_grad():
                caption = describe_frame_with_prompt(
                    frameData.frame, 
                    prompt, 
                    models['processor'], 
                    models['model']
                )

            all_caption += f" {caption} [{box.class_name}]"

        # LLM processing
        vlm_description = all_caption
        question = question_data["question"]
        choices = question_data["choices"]

        with torch.no_grad():
            final_answer = lm_generate(
                llm=models['llm'],
                tokenizer=models['tokenizer'],
                retriever=models['retriever'],
                reranker=models['reranker'],
                vlm_description=vlm_description,
                question=question + "\n" + "\n".join(choices),
            )

        end_time = time.time()
        processing_time = end_time - start_time
        
        clean_answer = final_answer.strip()[0] if final_answer.strip() else "A"
        
        thread_safe_print(f"âœ… [{question_index:3d}/{total_questions}] {question_data['id']}: {clean_answer} ({processing_time:.1f}s)")
        
        return {
            'id': question_data['id'],
            'answer': clean_answer,
            'processing_time': processing_time,
            'index': question_index
        }
        
    except Exception as e:
        end_time = time.time()
        processing_time = end_time - start_time
        
        thread_safe_print(f"âŒ [{question_index:3d}/{total_questions}] {question_data['id']}: {str(e)[:50]}... ({processing_time:.1f}s)")
        
        return {
            'id': question_data['id'],
            'answer': "A",
            'processing_time': processing_time,
            'index': question_index
        }

def save_temp_results(results, temp_file_path):
    """LÆ°u káº¿t quáº£ táº¡m thá»i"""
    with file_lock:
        sorted_results = sorted(results, key=lambda x: x['index'])
        csv_data = [{'id': r['id'], 'answer': r['answer']} for r in sorted_results]
        
        with open(temp_file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['id', 'answer'])
            writer.writeheader()
            writer.writerows(csv_data)
        
        thread_safe_print(f"ğŸ’¾ Backup: {len(results)} káº¿t quáº£ -> {temp_file_path}")

def main():
    """HÃ m chÃ­nh tá»‘i Æ°u tá»‘c Ä‘á»™ cho 405 cÃ¢u há»i"""
    
    print("="*60)
    print("ğŸš€ ZALO CHALLENGE 2025 - SPEED OPTIMIZED VERSION")
    print("="*60)
    
    # Load data
    thread_safe_print("ğŸ“– Äá»c dá»¯ liá»‡u tá»« public_test.json...")
    with open('public_test/public_test.json', 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    questions = test_data['data']
    thread_safe_print(f"ğŸ“Š TÃ¬m tháº¥y {len(questions)} cÃ¢u há»i cáº§n xá»­ lÃ½")
    
    # Load models
    models = load_models_fast()
    
    # Cáº¥u hÃ¬nh tá»‘i Æ°u cho 405 cÃ¢u há»i
    max_workers = 3  # Sá»‘ luá»“ng song song
    thread_safe_print(f"ğŸ”§ Cáº¥u hÃ¬nh: {max_workers} luá»“ng song song, skip frames x2")
    
    # Äá»c file backup má»›i nháº¥t
    backup_file = 'public_test/backup_140.csv'
    backup_df = pd.read_csv(backup_file)

    # Giáº£ sá»­ backup_df Ä‘á»c tá»« backup CSV
    results = backup_df.to_dict(orient='records')

    # Lá»c cÃ¢u chÆ°a xá»­ lÃ½
    answered_ids = set(r['id'] for r in results)
    args_list = []
    for i, question in enumerate(questions, 1):
        if question['id'] in answered_ids:
            continue
        args_list.append((question, models, i, len(questions)))

    total_time = 0
    start_total = time.time()
    
    thread_safe_print(f"ğŸ¯ Báº¯t Ä‘áº§u xá»­ lÃ½ {len(questions)} cÃ¢u há»i...")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_args = {executor.submit(process_single_question_fast, args): args for args in args_list}
        
        completed_count = 0
        
        for future in as_completed(future_to_args):
            try:
                result = future.result()
                results.append(result)
                completed_count += 1
                total_time += result['processing_time']
                
                # Progress update má»—i 5 cÃ¢u há»i
                if completed_count % 5 == 0:
                    avg_time = total_time / completed_count
                    remaining = len(questions) - completed_count
                    estimated_remaining = (remaining * avg_time) / max_workers
                    throughput = completed_count / ((time.time() - start_total) / 60)  # cÃ¢u/phÃºt
                    
                    thread_safe_print(f"ğŸ“ˆ {completed_count:3d}/{len(questions)} | "
                                    f"TB: {avg_time:4.1f}s | "
                                    f"Tá»‘c Ä‘á»™: {throughput:4.1f} cÃ¢u/phÃºt | "
                                    f"CÃ²n láº¡i: {estimated_remaining/60:4.1f} phÃºt")
                
                # Backup má»—i 20 cÃ¢u há»i
                if completed_count % 20 == 0:
                    temp_file = f'public_test/backup_{completed_count}.csv'
                    save_temp_results(results, temp_file)
                    
            except Exception as exc:
                thread_safe_print(f'âŒ Lá»—i: {exc}')
    
    end_total = time.time()
    total_wall_time = end_total - start_total
    
    # LÆ°u káº¿t quáº£ cuá»‘i cÃ¹ng
    output_path = 'public_test/submission.csv'
    thread_safe_print(f"\nğŸ’¾ LÆ°u káº¿t quáº£ cuá»‘i cÃ¹ng: {output_path}")
    
    sorted_results = sorted(results, key=lambda x: x['index'])
    csv_data = [{'id': r['id'], 'answer': r['answer']} for r in sorted_results]
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['id', 'answer'])
        writer.writeheader()
        writer.writerows(csv_data)
    
    # Thá»‘ng kÃª cuá»‘i
    print("\n" + "="*60)
    print("ğŸ‰ HOÃ€N THÃ€NH!")
    print("="*60)
    thread_safe_print(f"ğŸ“ File káº¿t quáº£: {output_path}")
    thread_safe_print(f"ğŸ“Š Tá»•ng cÃ¢u há»i: {len(results)}")
    thread_safe_print(f"â±ï¸  Thá»i gian thá»±c táº¿: {total_wall_time/60:.1f} phÃºt")
    thread_safe_print(f"ğŸ“ˆ Thá»i gian TB/cÃ¢u: {total_time/len(results):.1f} giÃ¢y")
    thread_safe_print(f"ğŸš€ Tá»‘c Ä‘á»™: {len(results)/(total_wall_time/60):.1f} cÃ¢u há»i/phÃºt")
    
    # Æ¯á»›c tÃ­nh cho cÃ¡c dataset khÃ¡c
    if len(results) < 405:
        estimated_405 = (405 * total_wall_time/len(results)) / 60
        thread_safe_print(f"ğŸ’¡ Æ¯á»›c tÃ­nh cho 405 cÃ¢u há»i: {estimated_405:.1f} phÃºt")
    
    print("="*60)

if __name__ == "__main__":
    main()
