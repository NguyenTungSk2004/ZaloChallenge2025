# ๐ Hฦฐแปng dแบซn build llama-cpp-python cรณ CUDA cho Windows

> รp dแปฅng cho mรดi trฦฐแปng `rag` (Python 3.11, CUDA 12.6, GPU RTX 2060 trแป lรชn, AE lฦฐu รฝ xem mรกy cรi CUDA, python gรฌ nhรฉ)

---

## 1๏ธโฃ Chuแบฉn bแป mรดi trฦฐแปng

### Gแปก sแบกch Torch cลฉ

pip uninstall torch torchvision torchaudio -y
Cรi CMake (>=3.31.7)
Tแบฃi tแบกi: https://github.com/Granddyser/windows-llama-cpp-python-cuda-guide
Trong quรก trรฌnh cรi: chแปn โAdd CMake to PATH for all usersโ

Kiแปm tra CUDA

echo %CUDA_PATH%
nvcc --version
ฤแบฃm bแบฃo xuแบฅt ra kiแปu nhฦฐ:

nvcc: NVIDIA (R) Cuda compiler driver
Cuda compilation tools, release 12.6, V12.6.20
2๏ธโฃ Clone vร build llama-cpp-python

git clone https://github.com/abetlen/llama-cpp-python.git
cd llama-cpp-python
git submodule update --init --recursive

3๏ธโฃ Cแบฅu hรฌnh build GPU
GPU	Compute Capability	CMake option
GTX 10xx (Pascal)	6.1	-DCMAKE_CUDA_ARCHITECTURES=61
RTX 20xx (Turing)	7.5	-DCMAKE_CUDA_ARCHITECTURES=75 โ
RTX 30xx (Ampere)	8.6	-DCMAKE_CUDA_ARCHITECTURES=86
RTX 40xx (Ada)	8.9	-DCMAKE_CUDA_ARCHITECTURES=89

4๏ธโฃ Build vร cรi ฤแบทt
conda activate rag
set FORCE_CMAKE=1
set CMAKE_ARGS=-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=75
pip install -e . --force-reinstall --no-cache-dir
Nแบฟu build thรnh cรดng, ae sแบฝ thแบฅy thฦฐ mแปฅc build/ sinh ra trong llama-cpp-python/.

5๏ธโฃ Kiแปm tra GPU hoแบกt ฤแปng
python
Copy code
from llama_cpp import Llama
import llama_cpp

print(llama_cpp.__version__)
print(llama_cpp.llama_print_system_info())

llm = Llama(model_path="E:/Zalo Challenge 2025/Build_RAG/model/Phi-3-gguf/Phi-3-mini-4k-instruct-q4.gguf", n_gpu_layers=999) -> ฤรขy lร folder trรชn mรกy tรดi ae ฤแป แป ฤรขu nhแป ฤแปi nhรฉ
print("โ GPU LlamaCpp chแบกy OK")
Nแบฟu ae sแบฝ thแบฅy trong log cรณ dรฒng โCUDA = 1 | cuBLAS = 1โ lร GPU ฤรฃ ฤฦฐแปฃc kรญch hoแบกt ๐
