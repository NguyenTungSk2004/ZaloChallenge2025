# üöÄ H∆∞·ªõng d·∫´n build llama-cpp-python c√≥ CUDA cho Windows

> √Åp d·ª•ng cho m√¥i tr∆∞·ªùng `rag` (Python 3.11, CUDA 12.6, GPU RTX 2060 tr·ªü l√™n, AE l∆∞u √Ω xem m√°y c√†i CUDA, python g√¨ nh√©)

---

## 1Ô∏è‚É£ Chu·∫©n b·ªã m√¥i tr∆∞·ªùng

### G·ª° s·∫°ch Torch c≈©
```bash
pip uninstall torch torchvision torchaudio -y
C√†i CMake (>=3.31.7)
T·∫£i t·∫°i: https://github.com/Granddyser/windows-llama-cpp-python-cuda-guide
Trong qu√° tr√¨nh c√†i: ch·ªçn ‚ÄúAdd CMake to PATH for all users‚Äù

Ki·ªÉm tra CUDA
bash
Copy code
echo %CUDA_PATH%
nvcc --version
ƒê·∫£m b·∫£o xu·∫•t ra nh∆∞:

arduino
Copy code
nvcc: NVIDIA (R) Cuda compiler driver
Cuda compilation tools, release 12.6, V12.6.20
2Ô∏è‚É£ Clone v√† build llama-cpp-python
bash
Copy code
git clone https://github.com/abetlen/llama-cpp-python.git
cd llama-cpp-python
git submodule update --init --recursive
3Ô∏è‚É£ C·∫•u h√¨nh build GPU
GPU	Compute Capability	CMake option
GTX 10xx (Pascal)	6.1	-DCMAKE_CUDA_ARCHITECTURES=61
RTX 20xx (Turing)	7.5	-DCMAKE_CUDA_ARCHITECTURES=75 ‚úÖ
RTX 30xx (Ampere)	8.6	-DCMAKE_CUDA_ARCHITECTURES=86
RTX 40xx (Ada)	8.9	-DCMAKE_CUDA_ARCHITECTURES=89

4Ô∏è‚É£ Build v√† c√†i ƒë·∫∑t
bash
Copy code
conda activate rag
set FORCE_CMAKE=1
set CMAKE_ARGS=-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=75
pip install -e . --force-reinstall --no-cache-dir
N·∫øu build th√†nh c√¥ng, b·∫°n s·∫Ω th·∫•y th∆∞ m·ª•c build/ sinh ra trong llama-cpp-python/.

5Ô∏è‚É£ Ki·ªÉm tra GPU ho·∫°t ƒë·ªông
python
Copy code
from llama_cpp import Llama
import llama_cpp

print(llama_cpp.__version__)
print(llama_cpp.llama_print_system_info())

llm = Llama(model_path="E:/Zalo Challenge 2025/Build_RAG/model/Phi-3-gguf/Phi-3-mini-4k-instruct-q4.gguf", n_gpu_layers=999)
print("‚úÖ GPU LlamaCpp ch·∫°y OK")
N·∫øu b·∫°n th·∫•y trong log c√≥ d√≤ng ‚ÄúCUDA = 1 | cuBLAS = 1‚Äù l√† GPU ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t üéâ